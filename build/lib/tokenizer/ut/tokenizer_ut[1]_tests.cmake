add_test([=[TTokenizer.BasicTokenization]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.BasicTokenization]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.BasicTokenization]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:8 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.PreservesPosition]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.PreservesPosition]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.PreservesPosition]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:17 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.HandlesNumbers]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.HandlesNumbers]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.HandlesNumbers]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:28 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.SkipsNumbers]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.SkipsNumbers]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.SkipsNumbers]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:41 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.HandlesPunctuation]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.HandlesPunctuation]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.HandlesPunctuation]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:50 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.SkipsPunctuation]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.SkipsPunctuation]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.SkipsPunctuation]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:64 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.LowerCase]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.LowerCase]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.LowerCase]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:73 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.PreserveCase]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.PreserveCase]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.PreserveCase]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:82 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.MinTokenLength]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.MinTokenLength]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.MinTokenLength]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:94 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.EmptyInput]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.EmptyInput]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.EmptyInput]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:106 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.WhitespaceOnly]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.WhitespaceOnly]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.WhitespaceOnly]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:112 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.TokenizeToStrings]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.TokenizeToStrings]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.TokenizeToStrings]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:118 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.ToLower]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.ToLower]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.ToLower]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:128 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.ToUpper]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.ToUpper]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.ToUpper]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:134 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.Normalize]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.Normalize]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.Normalize]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:140 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.Trim]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.Trim]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.Trim]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:145 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.Split]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.Split]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.Split]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:151 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.Join]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.Join]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.Join]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:160 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.ComplexText]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.ComplexText]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.ComplexText]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:169 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
add_test([=[TTokenizer.HyphenatedWords]=]  /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut/tokenizer_ut [==[--gtest_filter=TTokenizer.HyphenatedWords]==] --gtest_also_run_disabled_tests)
set_tests_properties([=[TTokenizer.HyphenatedWords]=]  PROPERTIES DEF_SOURCE_LINE /Users/nyamerka/Desktop/info_search/lib/tokenizer/ut/tokenizer_ut.cpp:178 WORKING_DIRECTORY /Users/nyamerka/Desktop/info_search/build/lib/tokenizer/ut SKIP_REGULAR_EXPRESSION [==[\[  SKIPPED \]]==])
set(  tokenizer_ut_TESTS TTokenizer.BasicTokenization TTokenizer.PreservesPosition TTokenizer.HandlesNumbers TTokenizer.SkipsNumbers TTokenizer.HandlesPunctuation TTokenizer.SkipsPunctuation TTokenizer.LowerCase TTokenizer.PreserveCase TTokenizer.MinTokenLength TTokenizer.EmptyInput TTokenizer.WhitespaceOnly TTokenizer.TokenizeToStrings TTokenizer.ToLower TTokenizer.ToUpper TTokenizer.Normalize TTokenizer.Trim TTokenizer.Split TTokenizer.Join TTokenizer.ComplexText TTokenizer.HyphenatedWords)
